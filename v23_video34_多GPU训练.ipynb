{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8017dd4-3572-4e4a-a933-ed229e81436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1168466c-6fa5-4aaf-8f30-0e73a00b0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# 定义模型\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ee7cc0-979f-4d6f-bc94-8e2011e2618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667abd67-f88e-4de2-a5c7-0f125b27999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 权重: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "b1 梯度: None\n"
     ]
    }
   ],
   "source": [
    "new_params = get_params(params, utils.try_gpu(0))\n",
    "print('b1 权重:', new_params[1])\n",
    "print('b1 梯度:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40222824-882d-4990-9a03-9502266d092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    \"\"\"\n",
    "        将所有数据移动到同一设备后累加，并将加和结果覆盖每一个原数据\n",
    "    \"\"\"\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d751ef1-5ce5-4e23-9e5f-71aaf082e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allreduce之前：\n",
      " tensor([[1., 1.]], device='cuda:0') \n",
      " tensor([[2., 2.]])\n",
      "allreduce之后：\n",
      " tensor([[3., 3.]], device='cuda:0') \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "data = [torch.ones((1, 2), device=utils.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('allreduce之前：\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('allreduce之后：\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf2863e-5ff1-44a4-8cf1-5634f74bdbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "load into [device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "output: (tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]], device='cuda:0'), tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# 数据分发\n",
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:0')]\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c65089f-2110-4721-85a3-85eb77797c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    \"\"\"将X和y拆分到多个设备上\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (\n",
    "        nn.parallel.scatter(X, devices), \n",
    "        nn.parallel.scatter(y, devices)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91b79f8-e114-4ea2-8322-080080632e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量上实现多GPU训练\n",
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # 在每个GPU上分别计算损失\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(\n",
    "              X_shards, y_shards, device_params)]\n",
    "    for l in ls:  # 反向传播在每个GPU上分别执行\n",
    "        l.backward()\n",
    "    # 将每个GPU的所有梯度相加，并将其广播到所有GPU\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce(\n",
    "                [device_params[c][i].grad for c in range(len(devices))])\n",
    "    # 在每个GPU上分别更新模型参数\n",
    "    for param in device_params:\n",
    "        utils.sgd(param, lr, X.shape[0]) # 在这里，我们使用全尺寸的小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf4c17b-df3c-4756-ad87-ca03361a88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)\n",
    "    devices = [utils.try_gpu(i) for i in range(num_gpus)]\n",
    "    # 将模型参数复制到num_gpus个GPU\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            # 为单个小批量执行多GPU训练\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        # 在GPU0上评估模型\n",
    "        lenet_x = lambda x: lenet(x, device_params[0])\n",
    "        acc = utils.evaluate_accuracy_gpu(lenet_x, test_iter, devices[0])\n",
    "        print(f'epoch: {epoch + 1}, acc: {acc}')\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209536cf-8222-459f-beea-5a6e6992bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, acc: 0.1\n",
      "epoch: 2, acc: 0.6679\n",
      "epoch: 3, acc: 0.714\n",
      "epoch: 4, acc: 0.7633\n",
      "epoch: 5, acc: 0.7179\n",
      "epoch: 6, acc: 0.7434\n",
      "epoch: 7, acc: 0.8085\n",
      "epoch: 8, acc: 0.8111\n",
      "epoch: 9, acc: 0.818\n",
      "epoch: 10, acc: 0.834\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9198200a-7d91-4172-9930-c5f2c4d6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多GPU的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53d5303-8cf7-45e9-8030-952abfa352b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(utils.Residual(in_channels, out_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(utils.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大池化层\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e917d3ff-9dd3-4f9b-860d-47aa899b99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(10)\n",
    "# 获取GPU列表\n",
    "devices = utils.try_all_gpus()\n",
    "# 我们将在训练代码实现中初始化网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ece1f3f-81c3-40fb-92cf-764a0f6cc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)\n",
    "    devices = [utils.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # 在多个GPU上设置模型\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        acc = utils.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'epoch: {epoch + 1}, acc: {acc}')\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da004511-8e6b-4821-b21d-06ae8976b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, acc: 0.7889\n",
      "epoch: 2, acc: 0.8524\n",
      "epoch: 3, acc: 0.8733\n",
      "epoch: 4, acc: 0.8793\n",
      "epoch: 5, acc: 0.9006\n",
      "epoch: 6, acc: 0.8661\n",
      "epoch: 7, acc: 0.9101\n",
      "epoch: 8, acc: 0.909\n",
      "epoch: 9, acc: 0.9088\n",
      "epoch: 10, acc: 0.9056\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8199bf9-6ea6-43ec-a099-d6347b0dda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
