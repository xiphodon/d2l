{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5c3d1b-c4dc-409d-aa3a-a377588e28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import utils\n",
    "import torchvision\n",
    "from torchvision.io import image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bedc58-fddc-4df6-ab6a-146f268dbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d5f873-3e34-4582-b9b4-3869288983db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", utils.EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                                            ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db7c2a6-b9f9-4353-b2b2-89e5e219dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164da0da-0aaf-4519-9d2f-cb09e9985960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens定义为长度为8的2个输入序列，其中每个词元是词表的索引\n",
    "tokens = torch.randint(low=0, high=vocab_size, size=(2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df02c93-6afa-4103-8055-d967262f9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e0d8f-f96a-4437-adcd-fd3ca4a88b5f",
   "metadata": {},
   "source": [
    "# 掩蔽语言模型（Masked Language Modeling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1151d4c1-fc66-47ce-beed-99f55c8044b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择15%的词元作为预测的掩蔽词元\n",
    "\n",
    "#1, 80%时间为特殊的“<mask>“词元（例如，“this movie is great”变为“this movie is<mask>”；\n",
    "\n",
    "#2, 10%时间为随机词元（例如，“this movie is great”变为“this movie is drink”）；\n",
    "\n",
    "#3, 10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45be67dd-768f-42c6-894e-68bd1ed6a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的掩蔽语言模型任务\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        # X:BERTEncoder的编码结果, pred_positions：用于预测的词元位置\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 假设batch_size=2，num_pred_positions=3\n",
    "        # 那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        # masked_X.shape [2, 3, 768]\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdcdad6-206d-4508-a379-68639f475c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389efa4e-4390-4493-9b5e-b03b14a55b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过掩码下的预测词元的真实标签mlm_Y，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81757055-3741-4b9a-87b6-e7972612eacc",
   "metadata": {},
   "source": [
    "# 下一句预测（Next Sentence Prediction）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd75ee8b-e9c8-413a-9b86-a93c122feefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT在预训练中考虑了一个二元分类任务————下一句预测。\n",
    "# 在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50915593-9307-4c99-aa50-03692796e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测第二个句子是否是BERT输入序列中第一个句子的下一个句子\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X的形状：(batchsize,num_hiddens)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695c173b-8893-4114-8842-b86f70f0147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 768])\n",
      "torch.Size([2, 6144])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_X.shape)\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "print(encoded_X.shape)\n",
    "print(nsp_Y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8466cd87-e24f-4472-8e9e-78f86298b386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算两个二元分类的交叉熵损失\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fc35c-be7b-4bb5-acf9-84e70357ee60",
   "metadata": {},
   "source": [
    "# 整合代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144c3380-49e1-434a-9a81-5bc4c3d91eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。\n",
    "# 现在我们可以通过实例化三个类BERTEncoder、MaskLM和NextSentencePred来定义BERTModel类。\n",
    "# 前向推断返回编码后的BERT表示encoded_X、掩蔽语言模型预测mlm_Y_hat和下一句预测nsp_Y_hat。\n",
    "\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, \n",
    "                                   dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db908d80-8e0c-45a5-8ddd-1e821a86c7fa",
   "metadata": {},
   "source": [
    "# BERT预训练数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6fcf568-7a8b-4859-be6a-915297d89605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText-2数据集（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；\n",
    "\n",
    "# 在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。\n",
    "# 保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d063ceb4-b390-4474-a6c4-997f887f19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = Path(r'../data/wikitext-2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8213ace8-b6e3-4cae-a849-6d081a9ecf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    file_path = data_dir / 'wiki.train.tokens'\n",
    "    with open(file_path.as_posix(), 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62da2ff3-36ad-4bcd-b3ca-f962de158913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成下一句预测任务的数据\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c4c8c3-59ac-4e90-aeb8-6f8b2d0b35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成用于下一句预测的训练样本\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d1742a2-b32d-4d24-94fe-1744b149c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成遮蔽语言模型任务的数据——替换mlm中需要mask的词元\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # candidate_pred_positions：候选预测位置，不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测）\n",
    "    # num_mlm_preds：指示预测的数量（选择15%要预测的随机词元）\n",
    "    \n",
    "    # 为遮蔽语言模型的输入创建新的词元副本，后续mlm_input_tokens可能包含替换的“<mask>”或随机词元\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8942040-cf2f-4b26-801e-919287a54933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成遮蔽语言模型任务的数据\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # tokens是一个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45677310-76e8-4d23-bba8-e51af3cccbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成BERT输入数据\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    # examples包含来自两个预训练任务的辅助函数_get_nsp_data_from_paragraph和_get_mlm_data_from_tokens的输出。\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for token_ids, pred_positions, mlm_pred_label_ids, segments, is_next in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4470ccf-55f5-4ac1-a10e-3b78a573e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        paragraphs = [utils.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = utils.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, \n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx], self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2972d5-735b-4e18-9dc8-cb55503d3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = utils.get_dataloader_workers()\n",
    "    paragraphs = _read_wiki(data_dir_path)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4453823e-121d-4c09-8334-700d0a6af645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape, pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape, nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b794226-ce19-4ec6-8dda-a1a05a8f98f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a2951-6156-44d0-9e28-aee3afdc29c1",
   "metadata": {},
   "source": [
    "### 为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：pip install nltk。在代码中先import nltk。然后下载Punkt语句词元分析器：nltk.download('punkt')。要拆分句子，比如sentences = 'This is great ! Why not ?'，调用nltk.tokenize.sent_tokenize(sentences)将返回两个句子字符串的列表：['This is great !', 'Why not ?']。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0e3a55-841a-4b0b-8dc4-ddaba364c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们加载WikiText-2数据集作为小批量的预训练样本，用于遮蔽语言模型和下一句预测。\n",
    "# 批量大小是512，BERT输入序列的最大长度是64。注意，在原始BERT模型中，最大长度是512。\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9154b-837e-450f-8118-f803c85a4cb3",
   "metadata": {},
   "source": [
    "# 预训练BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23067e65-1796-4861-b578-d8133e3cdd71",
   "metadata": {},
   "source": [
    "原始BERT (Devlin et al., 2018)有两个不同模型尺寸的版本。基本模型（bert_base）使用12层（Transformer编码器块），768个隐藏单元（隐藏大小）和12个自注意头。大模型（bert_large）使用24层，1024个隐藏单元和16个自注意头。值得注意的是，前者有1.1亿个参数，后者有3.4亿个参数。\n",
    "为了便于演示，我们定义了一个小的BERT，使用了2层、128个隐藏单元和2个自注意头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a8dbf80-5e79-464d-a6c2-d77a7d146564",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTModel(len(vocab), num_hiddens=128, norm_shape=[128], ffn_num_input=128, ffn_num_hiddens=256, num_heads=2, num_layers=2, \n",
    "                dropout=0.2, key_size=128, query_size=128, value_size=128, hid_in_features=128, mlm_in_features=128, nsp_in_features=128)\n",
    "devices = utils.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e00fd38f-5c65-4f07-9a30-945b68f70b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT预训练的最终损失是遮蔽语言模型损失和下一句预测损失的和。\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y):\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x.reshape(-1), pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) * mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e9e7d7c-08fe-415b-82c3-e552a47fe5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入num_steps指定了训练的迭代步数\n",
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
    "    mlm_l_sum = nsp_l_sum = tokens_num = count = 0\n",
    "    time_used_list = list()\n",
    "    num_steps_reached = False\n",
    "    step = 0\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            time_0 = time.time()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                                                   pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            mlm_l_sum += mlm_l\n",
    "            nsp_l_sum += nsp_l\n",
    "            tokens_num += tokens_X.shape[0]\n",
    "            count += 1\n",
    "            time_used_list.append(time.time() - time_0)\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(f'step: {step + 1}/{num_steps}, MLM loss {mlm_l_sum / count:.3f}, NSP loss {nsp_l_sum / count:.3f}')\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {mlm_l_sum / count:.3f}, NSP loss {nsp_l_sum / count:.3f}')\n",
    "    print(f'{tokens_num / sum(time_used_list):.1f} sentence pairs/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce52b4be-d164-450c-b428-3d00bde96aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50/500, MLM loss 5.676, NSP loss 0.770\n",
      "step: 100/500, MLM loss 5.355, NSP loss 0.733\n",
      "step: 150/500, MLM loss 5.211, NSP loss 0.721\n",
      "step: 200/500, MLM loss 5.132, NSP loss 0.717\n",
      "step: 250/500, MLM loss 5.087, NSP loss 0.713\n",
      "step: 300/500, MLM loss 5.053, NSP loss 0.711\n",
      "step: 350/500, MLM loss 5.032, NSP loss 0.709\n",
      "step: 400/500, MLM loss 5.023, NSP loss 0.708\n",
      "step: 450/500, MLM loss 5.013, NSP loss 0.707\n",
      "step: 500/500, MLM loss 5.007, NSP loss 0.706\n",
      "MLM loss 5.007, NSP loss 0.706\n",
      "3342.5 sentence pairs/sec on [device(type='cuda', index=0)]\n"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac1e23-01e6-4a53-865d-85cf6351897a",
   "metadata": {},
   "source": [
    "# 用BERT表示文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "213e0af9-5a8f-458c-b10c-aed861c3aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95286871-5874-4626-9397-82453e6326d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([-0.3926, -1.2659,  2.3238], device='cuda:0', grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127a273-7cc3-4070-acf3-eeea2edd18d2",
   "metadata": {},
   "source": [
    "在预训练BERT之后，我们可以用它来表示单个文本、文本对或其中的任何词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61196c3-73af-48f8-a647-87105ed87374",
   "metadata": {},
   "source": [
    "同一个词元在不同的上下文中具有不同的BERT表示。这支持BERT表示是上下文敏感的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc01fc-47ef-4f14-9a0f-325980b7f24e",
   "metadata": {},
   "source": [
    "# 针对序列级和词元级应用微调BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a91842-c2d1-4a44-ae2f-f5d9de059111",
   "metadata": {},
   "source": [
    "## 1、单文本分类\n",
    "    \n",
    "在单文本分类应用中，特殊分类标记“\\<cls\\>”的BERT表示对整个输入文本序列的信息进行编码。作为输入单个文本的表示，它将被送入到由全连接（稠密）层组成的小多层感知机中，以输出所有离散标签值的分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94002b95-25d0-40b9-afb3-602b7de7ecfa",
   "metadata": {},
   "source": [
    "![v54_1](../data/img/v54_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef38e65-f03b-478d-abdc-bf51c824f36c",
   "metadata": {},
   "source": [
    "## 2、文本对分类或回归\n",
    "![v54_2](../data/img/v54_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a473ac4-de41-460f-80d8-2801a8fd4e44",
   "metadata": {},
   "source": [
    "# 3、文本标注\n",
    "![v54_3](../data/img/v54_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd2ddd-0aef-40cc-b0d6-cbded86b85db",
   "metadata": {},
   "source": [
    "# 4、问答\n",
    "![v54_4](../data/img/v54_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae8b1a-0110-4341-af7f-df3f32ccd74d",
   "metadata": {},
   "source": [
    "![v54_5](../data/img/v54_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a52eb-310f-4b50-8913-0e42b499ae92",
   "metadata": {},
   "source": [
    "# 自然语言推断：微调BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31439fa5-6e92-4eaf-b81f-7fe2e0971a15",
   "metadata": {},
   "source": [
    "自然语言推断（natural language inference）主要研究 假设（hypothesis）是否可以从前提（premise）中推断出来， 其中两者都是文本序列。 换言之，自然语言推断决定了一对文本序列之间的逻辑关系。这类关系通常分为三种类型：\n",
    "\n",
    "- 蕴涵（entailment）：假设可以从前提中推断出来。\n",
    "\n",
    "- 矛盾（contradiction）：假设的否定可以从前提中推断出来。\n",
    "\n",
    "- 中性（neutral）：所有其他情况。\n",
    "\n",
    "自然语言推断也被称为识别文本蕴涵任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0f68df3-ee13-433a-97d1-c916da626cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = Path(fr'../data/{pretrained_model}.torch')\n",
    "    if not data_dir.exists:\n",
    "        raise Exception(f'dir:{data_dir.as_posix()} is not exists.')\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = utils.Vocab()\n",
    "    vocab.idx_to_token = json.load(open((data_dir / 'vocab.json').as_posix()))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    bert = BERTModel(len(vocab), num_hiddens, norm_shape=[256], ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, num_heads=4, num_layers=2, dropout=0.2, \n",
    "                     max_len=max_len, key_size=256, query_size=256, value_size=256, hid_in_features=256, mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load((data_dir / 'pretrained.params').as_posix()))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c721edc4-718e-493b-868a-b88852655819",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = utils.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model('bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4, \n",
    "                                    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43dc0b1-b167-4a60-a35e-dabc9c79a29f",
   "metadata": {},
   "source": [
    "### 微调BERT的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "854f3299-659a-45ce-b066-6512b48ab537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福自然语言推断（SNLI）数据集\n",
    "snli_data_dir = Path(r'../data/snli_1.0')\n",
    "# 列名\n",
    "# gold_label, sentence1_binary_parse, sentence2_binary_parse, sentence1_parse, sentence2_parse, sentence1, sentence2, captionID, pairID, label1, label2, label3, label4, label5\n",
    "\n",
    "def read_snli(data_dir: Path, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    def extract_text(s):\n",
    "        # 删除我们不会使用的信息\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # 用一个空格替换两个或多个连续的空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = data_dir / ('snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name.as_posix(), 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1976dac-0cff-468e-90cc-33a3a036ed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "train_data = read_snli(snli_data_dir, is_train=True)\n",
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7e4b4c6-aefe-40ef-a05a-45197abc0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "# 训练集和测试集中的三个标签“蕴涵”“矛盾”和“中性”数量是平衡的。\n",
    "test_data = read_snli(snli_data_dir, is_train=False)\n",
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e2b4ace-6919-4a1b-9bed-73158cce015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = utils.tokenize(dataset[0], token='word')\n",
    "        all_hypothesis_tokens = utils.tokenize(dataset[1], token='word')\n",
    "        if vocab is None:\n",
    "            self.vocab = utils.Vocab(all_premise_tokens + all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([utils.truncate_pad(self.vocab[line], self.num_steps, self.vocab['<pad>']) for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d2eb0db-2ddc-4fb4-b6ab-0848c9b287ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"加载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = utils.get_dataloader_workers()\n",
    "    train_data = read_snli(snli_data_dir, True)\n",
    "    test_data = read_snli(snli_data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2421747f-a759-4259-a1f9-bf03085d3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(batch_size=128, num_steps=50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b98d40b-28b0-4407-aef0-4041c573e29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b32d2304-cb69-4038-befd-aa1bcda69eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = None\n",
    "test_iter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6374c052-b401-4bdf-806b-2bdffb4611d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做成 bert 数据集\n",
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [\n",
    "            [p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "                *[utils.tokenize([s.lower() for s in sentences], token='word') for sentences in dataset[:2]]\n",
    "            )\n",
    "        ]\n",
    "        print('- SNLIBERTDataset dataset loaded -')\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.all_token_ids, self.all_segments, self.valid_lens = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('preprocess over, read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        # pool = multiprocessing.Pool(4)  # 使用多进程\n",
    "        # out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        out = map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = list()\n",
    "        all_segments = list()\n",
    "        valid_lens = list()\n",
    "        for token_ids, segments, valid_len in out:\n",
    "            all_token_ids.append(token_ids)\n",
    "            all_segments.append(segments)\n",
    "            valid_lens.append(valid_len)\n",
    "        return torch.tensor(all_token_ids, dtype=torch.long), torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens)\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        p_tokens, h_tokens = self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        p_h_tokens_len = len(p_tokens) + len(h_tokens)\n",
    "        if p_h_tokens_len > self.max_len - 3:\n",
    "            # 需弹出的数量\n",
    "            pop_num = p_h_tokens_len - (self.max_len - 3)\n",
    "            # 每个应从尾部去掉的数量\n",
    "            half_pop_num = pop_num//2\n",
    "            if pop_num % 2 == 0:\n",
    "                p_tokens = p_tokens[:-half_pop_num]\n",
    "                h_tokens = h_tokens[:-half_pop_num]\n",
    "            else:\n",
    "                p_tokens = p_tokens[:-half_pop_num]\n",
    "                h_tokens = h_tokens[:-(half_pop_num+1)]\n",
    "        return p_tokens, h_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63c0075f-2d14-4f23-954a-8f1324177cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- SNLIBERTDataset dataset loaded -\n",
      "preprocess over, read 549367 examples\n",
      "- SNLIBERTDataset dataset loaded -\n",
      "preprocess over, read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, utils.get_dataloader_workers()\n",
    "train_set = SNLIBERTDataset(read_snli(snli_data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(read_snli(snli_data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90106b12-9b74-4e87-a86a-173226a5cdbd",
   "metadata": {},
   "source": [
    "# 微调BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79184196-1fc2-4c3f-8061-82cf761ad961",
   "metadata": {},
   "source": [
    "用于自然语言推断的微调BERT只需要一个额外的多层感知机，该多层感知机由两个全连接层组成（请参见下面BERTClassifier类中的self.hidden和self.output）。这个多层感知机将特殊的“\\<cls\\>”词元的BERT表示进行了转换，该词元同时编码前提和假设的信息为自然语言推断的三个输出：蕴涵、矛盾和中性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f55217dc-2579-43a7-855f-d21107c78101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0866bf-716d-46b8-9733-b7b331282da1",
   "metadata": {},
   "source": [
    "在BERT微调的常见实现中，只有额外的多层感知机（net.output）的输出层的参数将从零开始学习。预训练BERT编码器（net.encoder）和额外的多层感知机的隐藏层（net.hidden）的所有参数都将进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0433081f-ba33-4ab6-b21a-fb4a018a9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "180f4661-bcf8-4c06-bd64-bfc515e36270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: [device(type='cuda', index=0)], [2024-09-14 16:41:57]\n",
      "epoch: 1/5, loss: 140.799, train_acc: 0.624, test_acc: 0.705, epoch_time: [0:00:39:53]\n",
      "epoch: 2/5, loss: 118.024, train_acc: 0.708, test_acc: 0.737, epoch_time: [0:00:39:32]\n",
      "epoch: 3/5, loss: 107.830, train_acc: 0.737, test_acc: 0.751, epoch_time: [0:00:41:28]\n",
      "epoch: 4/5, loss: 100.259, train_acc: 0.758, test_acc: 0.758, epoch_time: [0:00:41:55]\n",
      "epoch: 5/5, loss: 94.561, train_acc: 0.774, test_acc: 0.766, epoch_time: [0:00:41:56]\n",
      "*** training speed: 1.3 examples/sec on [device(type='cuda', index=0)], all_time: [0:03:24:47] ***\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "utils.train_gpus(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e97a4567-1a93-4575-a041-467442a9af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict_snli(net, vocab, premise_bat, hypothesis_bat, max_len, devices):\n",
    "    \"\"\"\n",
    "        推断前提和假设之间的逻辑关系\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    token_ids = list()\n",
    "    segments = list()\n",
    "    valid_len = list()\n",
    "    for p_tokens, h_tokens in zip(premise_bat, hypothesis_bat):\n",
    "        # 截断到定长内，为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "        # 拼接<cls>/<sep>形成bert的tokens和segments\n",
    "        _tokens, _segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        # 填充<pad>\n",
    "        _token_ids = vocab[_tokens] + [vocab['<pad>']] * (max_len - len(_tokens))\n",
    "        _segments = _segments + [0] * (max_len - len(_segments))\n",
    "        _valid_len = len(_tokens)\n",
    "       \n",
    "        # print(_token_ids)\n",
    "        # print(_segments)\n",
    "        # print(_valid_len)\n",
    "        \n",
    "        token_ids.append(torch.tensor(_token_ids))\n",
    "        segments.append(torch.tensor(_segments))\n",
    "        valid_len.append(torch.tensor(_valid_len))\n",
    "    \n",
    "    token_ids = torch.stack(token_ids, dim=0).to(devices[0])\n",
    "    segments = torch.stack(segments, dim=0).to(devices[0])\n",
    "    valid_len = torch.stack(valid_len, dim=0).to(devices[0])\n",
    "    \n",
    "    predict_label = torch.argmax(net(inputs=(token_ids, segments, valid_len)), dim=1)\n",
    "    print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "858b7f42-6222-4c86-9df2-3d9aa7d355a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_list = [\n",
    "    ['I love you very much .', 'I have feelings for you .'],\n",
    "    ['he likes to jump .', 'he likes sports .'],\n",
    "    ['This burger is delicious .', 'The sky is blue .'],\n",
    "    ['I am very hungry today .', 'I can not eat anything today .'],\n",
    "    ['I am full of energy .', 'I am tired .']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fd07bfe-f962-4e30-896c-f4f1f3bf330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['I', 'love', 'you', 'very', 'much', '.'],\n",
       "  ['he', 'likes', 'to', 'jump', '.'],\n",
       "  ['This', 'burger', 'is', 'delicious', '.'],\n",
       "  ['I', 'am', 'very', 'hungry', 'today', '.'],\n",
       "  ['I', 'am', 'full', 'of', 'energy', '.']],\n",
       " [['I', 'have', 'feelings', 'for', 'you', '.'],\n",
       "  ['he', 'likes', 'sports', '.'],\n",
       "  ['The', 'sky', 'is', 'blue', '.'],\n",
       "  ['I', 'can', 'not', 'eat', 'anything', 'today', '.'],\n",
       "  ['I', 'am', 'tired', '.']])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sentence_list = list()\n",
    "h_sentence_list = list()\n",
    "for pair_sentence_list in test_sentence_list:\n",
    "    p_sentence_list.append(pair_sentence_list[0].split())\n",
    "    h_sentence_list.append(pair_sentence_list[1].split())\n",
    "p_sentence_list, h_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4492bce4-c0d2-4ac0-84ca-2925a4e67ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predict_label = bert_predict_snli(net, vocab, p_sentence_list, h_sentence_list, max_len, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7123d5f-c19e-4d90-a554-758538b5a770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
